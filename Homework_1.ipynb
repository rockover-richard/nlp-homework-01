{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:MidnightBlue\"> Answers for Homework 1 </span>\n",
    "### <span style=\"color:DarkSlateBlue\"> Byung Kim<br>for CS74040: NLP Fall 2019 by Prof. Alla Rozovskaya<br>Due: 10/03/2019</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing\n",
    "import training\n",
    "import testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightCoral\"> Question 1:</span>\n",
    "#### <em>How many word types (unique words) are there in the training corpus? Please include the padding symbols and the unknown token.</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, I preprocess the training corpus\n",
    "train_fp = 'data/brown-train.txt'\n",
    "train_list = preprocessing.preprocess_train(train_fp)\n",
    "# Then I build a dictionary from the preprocessed data\n",
    "train_dict = preprocessing.build_dict(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15031 word types in the training corpus.\n"
     ]
    }
   ],
   "source": [
    "print('There are', len(train_dict), 'word types in the training corpus.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightCoral\"> Question 2:</span>\n",
    "#### <em>How many word tokens are there in the training corpus?</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 498474 word tokens in the training corpus.\n"
     ]
    }
   ],
   "source": [
    "print('There are', len(train_list), 'word tokens in the training corpus.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightCoral\"> Question 3:</span>\n",
    "#### <em>What percentage of word tokens and word types in each of the test corpora did not occur in training (before you mapped the unknown words to &lt;unk&gt; in training and test data)?</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, the filepaths for the test data\n",
    "test_fp = 'data/brown-test.txt'\n",
    "learner_fp = 'data/learner-test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The add_s_tag function in preprocessing.py creates a list with <s> and </s> tags\n",
    "# I do this for all data: brown-train, brown-test, and learner-test\n",
    "train_temp = preprocessing.add_s_tag(train_fp)\n",
    "test_temp = preprocessing.add_s_tag(test_fp)\n",
    "learner_temp = preprocessing.add_s_tag(learner_fp)\n",
    "\n",
    "# I create a dictionary of brown-train and brown-test\n",
    "train_tdict = preprocessing.build_dict(train_temp)\n",
    "test_tdict = preprocessing.build_dict(test_temp)\n",
    "learner_tdict = preprocessing.build_dict(learner_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For word tokens in test data but not in training data\n",
    "def unseen_tokens_perc(test_list, train_dict):\n",
    "    unseen_sum = 0\n",
    "    for token in test_list:\n",
    "        if token not in train_dict:\n",
    "            unseen_sum += 1\n",
    "    return float(unseen_sum)/len(test_list)\n",
    "\n",
    "# For word types in test data but not in training data\n",
    "def unseen_type_perc(test_dict, train_dict):\n",
    "    unseen_sum = 0\n",
    "    for key in test_dict:\n",
    "        if key not in train_dict:\n",
    "            unseen_sum += 1\n",
    "    return float(unseen_sum)/len(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the brown-test data:\n",
      "The percentage of tokens not in the training data for brown-test is: 5.99%\n",
      "The percentage of word types not in the training data for brown-test is: 22.76%\n",
      "---------------------------------------------------------------------------------\n",
      "For the learner-test data:\n",
      "The percentage of tokens not in the training data for learner-test is: 5.05%\n",
      "The percentage of word types not in the training data for learner-test is: 16.35%\n"
     ]
    }
   ],
   "source": [
    "print('For the brown-test data:')\n",
    "print('The percentage of tokens not in the training data for brown-test is: {:.2%}'.format( \n",
    "      unseen_tokens_perc(test_temp, train_tdict)))\n",
    "print('The percentage of word types not in the training data for brown-test is: {:.2%}'.format( \n",
    "      unseen_type_perc(test_tdict, train_tdict)))\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('For the learner-test data:')\n",
    "print('The percentage of tokens not in the training data for learner-test is: {:.2%}'.format( \n",
    "      unseen_tokens_perc(learner_temp, train_tdict)))\n",
    "print('The percentage of word types not in the training data for learner-test is: {:.2%}'.format( \n",
    "      unseen_type_perc(learner_tdict, train_tdict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightCoral\"> Question 4:</span>\n",
    "#### <em>What percentage of bigrams (bigram types and bigram tokens) in each of the test corpora that did not occur in training (treat &lt;unk&gt; as a token that has been observed).</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, I have to preprocess the test data\n",
    "test_list = preprocessing.preprocess_test(test_fp, train_dict)\n",
    "learner_list = preprocessing.preprocess_test(learner_fp, train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then I build the bigram lists for all three data sets\n",
    "train_bigrams = training.bigrams(train_list)\n",
    "test_bigrams = training.bigrams(test_list)\n",
    "learner_bigrams = training.bigrams(learner_list)\n",
    "\n",
    "# And the bigram dictionary with counts for all three data sets\n",
    "train_bdict = training.bigram_dict(train_list)\n",
    "test_bdict = training.bigram_dict(test_list)\n",
    "learner_bdict = training.bigram_dict(learner_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the brown-test data:\n",
      "The percentage of bigram tokens not in the training data for brown-test is: 22.65%\n",
      "The percentage of bigram types not in the training data for brown-test is: 38.52%\n",
      "---------------------------------------------------------------------------------\n",
      "For the learner-test data:\n",
      "The percentage of bigram tokens not in the training data for learner-test is: 24.84%\n",
      "The percentage of bigram types not in the training data for learner-test is: 38.64%\n"
     ]
    }
   ],
   "source": [
    "# Recycling unseen_tokens_perc and unseen_type_perc from Question 3\n",
    "print('For the brown-test data:')\n",
    "print('The percentage of bigram tokens not in the training data for brown-test is: {:.2%}'.format( \n",
    "      unseen_tokens_perc(test_bigrams, train_bdict)))\n",
    "print('The percentage of bigram types not in the training data for brown-test is: {:.2%}'.format( \n",
    "      unseen_type_perc(test_bdict, train_bdict)))\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('For the learner-test data:')\n",
    "print('The percentage of bigram tokens not in the training data for learner-test is: {:.2%}'.format( \n",
    "      unseen_tokens_perc(learner_bigrams, train_bdict)))\n",
    "print('The percentage of bigram types not in the training data for learner-test is: {:.2%}'.format( \n",
    "      unseen_type_perc(learner_bdict, train_bdict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightCoral\"> Question 5 & 6:</span>\n",
    "#### <em>5. Compute the log probabilities of the following sentences under the three models (ignore capitalization and pad each sentence as described above). Please list all of the parameters required to compute the probabilities and show the complete calculation. Which of the parameters have zero values under each model?</em>\n",
    "* He was laughed off the screen .\n",
    "* There was no compulsion behind them .\n",
    "* I look forward to hearing your reply .\n",
    "\n",
    "#### <em>6. Compute the perplexities of each of the sentences above under each of the models.</em>\n",
    "\n",
    "The answers to both question 5 and 6 will be shown per model together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from testing import perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'he', 'was', 'laughed', 'off', 'the', 'screen', '.', '</s>']\n",
      "['<s>', 'there', 'was', 'no', 'compulsion', 'behind', 'them', '.', '</s>']\n",
      "['<s>', 'i', 'look', 'forward', 'to', 'hearing', 'your', 'reply', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# First, I preprocess the three sentences and put them into a list\n",
    "s1 = 'He was laughed off the screen .'\n",
    "s2 = 'There was no compulsion behind them .'\n",
    "s3 = 'I look forward to hearing your reply .'\n",
    "\n",
    "s1_list = preprocessing.sentence_preprocess(s1, lst=[])\n",
    "s2_list = preprocessing.sentence_preprocess(s2, lst=[])\n",
    "s3_list = preprocessing.sentence_preprocess(s3, lst=[])\n",
    "\n",
    "print(s1_list)\n",
    "print(s2_list)\n",
    "print(s3_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'he', 'was', 'laughed', 'off', 'the', 'screen', '.', '</s>']\n",
      "['<s>', 'there', 'was', 'no', '<unk>', 'behind', 'them', '.', '</s>']\n",
      "['<s>', 'i', 'look', 'forward', 'to', 'hearing', 'your', 'reply', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# We need a dictionary of the training set before <unk> was substituted\n",
    "# to see if any of these were unseen\n",
    "# train_tdict from Question 3 is as such\n",
    "s1_unk = preprocessing.unkify_test(s1_list, train_dict)\n",
    "s2_unk = preprocessing.unkify_test(s2_list, train_dict)\n",
    "s3_unk = preprocessing.unkify_test(s3_list, train_dict)\n",
    "\n",
    "print(s1_unk)\n",
    "print(s2_unk)\n",
    "print(s3_unk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Maroon\">Part A: Unigram Maximum Likelihood Model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we train the unigram model\n",
    "unigram_probs = training.unigram_train(train_list, train_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Parameters for Each Sentence:\n",
    "$l = \\frac{1}{M}\\sum_{i=1}^m log_2 p(s_i)$<br>\n",
    "s refers to the size of the corpus<br><br>\n",
    "$l_{s1} = \\frac{1}{9}log_2(p(<s>) * p(he) * p(was) * p(laughed) * p(off) * p(the) * p(screen) * p(.) * p(</s>))$<br>\n",
    "$= \\frac{1}{9}log_2(\\frac{c(<s>)}{s} * \\frac{c(he)}{s} * \\frac{c(was)}{s} * \\frac{c(laughed)}{s} * \\frac{c(off)}{s} * \\frac{c(the)}{s} * \\frac{c(screen}{s} * \\frac{c(.)}{s} * \\frac{c(</s>)}{s}) $<br><br>\n",
    "\n",
    "$l_{s2} = \\frac{1}{9}log_2(p(<s>) * p(there) * p(was) * p(no) * p(<unk>) * p(behind) * p(them) * p(.) * p(</s>))$<br>\n",
    "$= \\frac{1}{9}log_2(\\frac{c(<s>)}{s} * \\frac{c(there)}{s} * \\frac{c(was)}{s} * \\frac{c(no)}{s} * \\frac{c(<unk>)}{s} * \\frac{c(behind)}{s} * \\frac{c(them)}{s} * \\frac{c(.)}{s} * \\frac{c(</s>)}{s})$<br><br>\n",
    "\n",
    "$l_{s3} = \\frac{1}{10}log_2(p(<s>) * p(i) * p(look) * p(forward) * p(to) * p(hearing) * p(your) * p(reply) * p(.) * p(</s>))$<br>\n",
    "$= \\frac{1}{10}log_2(\\frac{c(<s>)}{s} * \\frac{c(i)}{s} * \\frac{c(look)}{s} * \\frac{c(forward)}{s} * \\frac{c(to)}{s} * \\frac{c(hearing)}{s} * \\frac{c(your)}{s} * \\frac{c(reply)}{s} * \\frac{c(.)}{s} * \\frac{c(</s>)}{s}$<br><br>\n",
    "\n",
    "None of these parameters are zero for the unigram model, as $<unk>$ is treated as an observed token<br>\n",
    "The function in unigram_predict prints all the individual log probabilities of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unigram log probability for <s> is -4.261\n",
      "The unigram log probability for he is -6.387\n",
      "The unigram log probability for was is -6.597\n",
      "The unigram log probability for laughed is -13.501\n",
      "The unigram log probability for off is -10.276\n",
      "The unigram log probability for the is -4.337\n",
      "The unigram log probability for screen is -15.02\n",
      "The unigram log probability for . is -4.486\n",
      "The unigram log probability for </s> is -4.261\n",
      "The log probability of sentence 1 in the Unigram ML Model is: -7.681\n",
      "The perplexity of sentence 1 in the Unigram ML Model is: 205.2\n",
      "---------------------------------------------------------------------\n",
      "The unigram log probability for <s> is -4.261\n",
      "The unigram log probability for there is -8.648\n",
      "The unigram log probability for was is -6.597\n",
      "The unigram log probability for no is -8.964\n",
      "The unigram log probability for <unk> is -5.237\n",
      "The unigram log probability for behind is -11.552\n",
      "The unigram log probability for them is -9.262\n",
      "The unigram log probability for . is -4.486\n",
      "The unigram log probability for </s> is -4.261\n",
      "The log probability of sentence 2 in the Unigram ML Model is: -7.030\n",
      "The perplexity of sentence 2 in the Unigram ML Model is: 130.7\n",
      "---------------------------------------------------------------------\n",
      "The unigram log probability for <s> is -4.261\n",
      "The unigram log probability for i is -7.268\n",
      "The unigram log probability for look is -11.075\n",
      "The unigram log probability for forward is -13.373\n",
      "The unigram log probability for to is -5.67\n",
      "The unigram log probability for hearing is -14.02\n",
      "The unigram log probability for your is -10.408\n",
      "The unigram log probability for reply is -14.069\n",
      "The unigram log probability for . is -4.486\n",
      "The unigram log probability for </s> is -4.261\n",
      "The log probability of sentence 3 in the Unigram ML Model is: -8.889\n",
      "The perplexity of sentence 3 in the Unigram ML Model is: 474.1\n",
      "---------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "s1_predict = testing.unigram_predict(s1_unk, unigram_probs)\n",
    "print('The log probability of sentence 1 in the Unigram ML Model is: {:.3f}'.format(s1_predict))\n",
    "print('The perplexity of sentence 1 in the Unigram ML Model is: {:.1f}'.format(perplexity(s1_predict)))\n",
    "print('---------------------------------------------------------------------')\n",
    "s2_predict = testing.unigram_predict(s2_unk, unigram_probs)\n",
    "print('The log probability of sentence 2 in the Unigram ML Model is: {:.3f}'.format(s2_predict))\n",
    "print('The perplexity of sentence 2 in the Unigram ML Model is: {:.1f}'.format(perplexity(s2_predict)))\n",
    "print('---------------------------------------------------------------------')\n",
    "s3_predict = testing.unigram_predict(s3_unk, unigram_probs)\n",
    "print('The log probability of sentence 3 in the Unigram ML Model is: {:.3f}'.format(s3_predict))\n",
    "print('The perplexity of sentence 3 in the Unigram ML Model is: {:.1f}'.format(perplexity(s3_predict)))\n",
    "print('---------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Maroon\">Part B: Bigram Maximum Likelihood Model</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function bigram_train in training.py automatically generates bigrams out of the training data\n",
    "# and we can just use train_bdict from Question 4,\n",
    "# which is just a dictionary of all the bigrams and their counts\n",
    "# thus, the following trains the bigram model\n",
    "bigram_probs = training.bigram_train(train_bdict, train_list, train_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Parameters for Each Sentence:\n",
    "$l_{s1} = \\frac{1}{9}log_2 (p(he|<s>) * p(was|he) * p(laughed|was) * p(off|laughed) * p(the|off) * p(screen|the) + p(.|screen) * p(</s>|.))$<br>\n",
    "$ = \\frac{1}{9}log_2 (\\frac{c(he|<s>)}{c(<s>)} * \\frac{c(was|he)}{c(he)} * \\frac{c(laughed|was)}{c(was)} * \\frac{c(off|laughed)}{c(laughed)} * \\frac{c(the|off)}{c(off)} * \\frac{c(screen|the)}{c(the)} + \\frac{c(.|screen)}{c(screen)} * \\frac{c(</s>|.)}{c(.)})$<br><br>\n",
    "\n",
    "$l_{s2} = \\frac{1}{9}log_2 (p(there|<s>) * p(was|there) * p(no|was) * p(<unk>|no) * p(behind|<unk>) * p(them|behind) * p(.|them) * p</s>|.))$<br>\n",
    "$= \\frac{1}{9}log_2 (\\frac{c(there|<s>)}{c(<s>)} * \\frac{c(was|there)}{c(there)} * \\frac{c(no|was)}{c(was)} * \\frac{c(<unk>|no)}{c(no)} * \\frac{c(behind|<unk>)}{c(<unk>)} * \\frac{c(them|behind)}{c(behind)} * \\frac{c(.|them)}{c(them)} * \\frac{c(</s>|.)}{c(.)})$<br><br>\n",
    "\n",
    "$l_{s3} = \\frac{1}{10}log_2 (p(i|<s>) * p(look|i) * p(forward|look) * p(to|forward) * p(hearing|to) * p(your|hearing) * p(reply|your) * p(.|reply) * p(</s>|.))$<br>\n",
    "$= \\frac{1}{10}log_2 (\\frac{c(i|<s>)}{c(<s>)} * \\frac{c(look|i)}{c(i)} * \\frac{c(forward|look)}{c(look)} * \\frac{c(to|forward)}{c(forward)} * \\frac{c(hearing|to)}{c(to)} * \\frac{c(your|hearing)}{c(hearing)} * \\frac{c(reply|your)}{c(your)} * \\frac{c(.|reply)}{c(reply)} * \\frac{c(</s>|.)}{c(.)})$<br><br>\n",
    "\n",
    "\n",
    "The function in bigram_predict prints all the individual log probabilities of the tokens.<br>\n",
    "The parameters that yield zero probability are listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'he'), ('he', 'was'), ('was', 'laughed'), ('laughed', 'off'), ('off', 'the'), ('the', 'screen'), ('screen', '.'), ('.', '</s>')]\n",
      "[('<s>', 'there'), ('there', 'was'), ('was', 'no'), ('no', '<unk>'), ('<unk>', 'behind'), ('behind', 'them'), ('them', '.'), ('.', '</s>')]\n",
      "[('<s>', 'i'), ('i', 'look'), ('look', 'forward'), ('forward', 'to'), ('to', 'hearing'), ('hearing', 'your'), ('your', 'reply'), ('reply', '.'), ('.', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "# I create bigrams of the three sentences\n",
    "s1_bigrams = training.bigrams(s1_unk)\n",
    "s2_bigrams = training.bigrams(s2_unk)\n",
    "s3_bigrams = training.bigrams(s3_unk)\n",
    "print(s1_bigrams)\n",
    "print(s2_bigrams)\n",
    "print(s3_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bigram log probability for ('<s>', 'he') is -3.608\n",
      "The bigram log probability for ('he', 'was') is -3.106\n",
      "This is an unobserved bigram. The probability for ('was', 'laughed') is 0.\n",
      "This is an unobserved bigram. The probability for ('laughed', 'off') is 0.\n",
      "The bigram log probability for ('off', 'the') is -2.422\n",
      "The bigram log probability for ('the', 'screen') is -13.005\n",
      "This is an unobserved bigram. The probability for ('screen', '.') is 0.\n",
      "The bigram log probability for ('.', '</s>') is 0.0\n",
      "The probability of sentence 1 in the Bigram ML Model is: 0.000\n",
      "The perplexity of sentence 1 in the Bigram ML Model is infinite.\n",
      "---------------------------------------------------------------------\n",
      "The bigram log probability for ('<s>', 'there') is -6.1\n",
      "The bigram log probability for ('there', 'was') is -1.706\n",
      "The bigram log probability for ('was', 'no') is -5.423\n",
      "The bigram log probability for ('no', '<unk>') is -5.208\n",
      "The bigram log probability for ('<unk>', 'behind') is -11.368\n",
      "The bigram log probability for ('behind', 'them') is -4.053\n",
      "The bigram log probability for ('them', '.') is -2.567\n",
      "The bigram log probability for ('.', '</s>') is 0.0\n",
      "The log probability of sentence 2 in the Bigram ML Model is: -4.047\n",
      "The perplexity of sentence 2 in the Bigram ML Model is: 16.5\n",
      "---------------------------------------------------------------------\n",
      "The bigram log probability for ('<s>', 'i') is -4.827\n",
      "The bigram log probability for ('i', 'look') is -11.66\n",
      "The bigram log probability for ('look', 'forward') is -5.852\n",
      "The bigram log probability for ('forward', 'to') is -1.854\n",
      "This is an unobserved bigram. The probability for ('to', 'hearing') is 0.\n",
      "This is an unobserved bigram. The probability for ('hearing', 'your') is 0.\n",
      "The bigram log probability for ('your', 'reply') is -8.52\n",
      "The bigram log probability for ('reply', '.') is -2.273\n",
      "The bigram log probability for ('.', '</s>') is 0.0\n",
      "The probability of sentence 3 in the Bigram ML Model is: 0.000\n",
      "The perplexity of sentence 3 in the Bigram ML Model is infinite.\n",
      "---------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "s1_bpred = testing.bigram_predict(s1_bigrams, bigram_probs, len(s1_unk))\n",
    "print('The probability of sentence 1 in the Bigram ML Model is: {:.3f}'.format(s1_bpred))\n",
    "print('The perplexity of sentence 1 in the Bigram ML Model is infinite.')\n",
    "print('---------------------------------------------------------------------')\n",
    "s2_bpred = testing.bigram_predict(s2_bigrams, bigram_probs, len(s2_unk))\n",
    "print('The log probability of sentence 2 in the Bigram ML Model is: {:.3f}'.format(s2_bpred))\n",
    "print('The perplexity of sentence 2 in the Bigram ML Model is: {:.1f}'.format(perplexity(s2_bpred)))\n",
    "print('---------------------------------------------------------------------')\n",
    "s3_bpred = testing.bigram_predict(s3_bigrams, bigram_probs, len(s3_unk))\n",
    "print('The probability of sentence 3 in the Bigram ML Model is: {:.3f}'.format(s3_bpred))\n",
    "print('The perplexity of sentence 3 in the Bigram ML Model is infinite.')\n",
    "print('---------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, in the bigram maximum likelihood model, sentence 1 and sentence 3 have 0 probability.<br>\n",
    "In sentence one, the following were 0 probability:\n",
    "* $p('was', 'laughed')$\n",
    "* $p('laughed', 'off')$\n",
    "* $p('screen', '.')$\n",
    "\n",
    "And in sentence three, the following were 0 probability:\n",
    "* $p('to', 'hearing')$\n",
    "* $p('hearing', 'your')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Maroon\">Part C: Bigram Model with Add-One Smoothing</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function add_one_train in training.py just reuses much of bigram_train code\n",
    "# but adds the proper 1 to the numerator and |V| to the denominator to all probabilities\n",
    "len_dict = len(train_dict)\n",
    "add_one_probs = training.add_one_train(train_bdict, train_list, train_dict, len_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Parameters for Each Sentence would be the same as the Bigram ML Model, and the probabilities would have the add-one / |V| :\n",
    "The biggest difference would be that $p('was', 'laughed'), p('laughed', 'off'), p('screen', '.')$ in sentence 1 and<br> $p('to', 'hearing'), p('hearing', 'your')$ in sentence 3 would not be zero, but instead would be $\\frac{1}{(w_{i-1}+|V|)}$.<br>\n",
    "\n",
    "$l_{s1} = \\frac{1}{9}log_2 (p(he|<s>) * p(was|he) * p(laughed|was) * p(off|laughed) * p(the|off) * p(screen|the) + p(.|screen) * p(</s>|.))$<br>\n",
    "$ = \\frac{1}{9}log_2 (\\frac{c(he|<s>)+1}{c(<s>)+|V|} * \\frac{c(was|he)+1}{c(he)+|V|} * \\frac{1}{c(was)+|V|} * \\frac{1}{c(laughed)+|V|} * \\frac{c(the|off)+1}{c(off)+|V|} * \\frac{c(screen|the)+1}{c(the)+|V|} + \\frac{1}{c(screen)+|V|} * \\frac{c(</s>|.)}{c(.)})$<br><br>\n",
    "\n",
    "$l_{s2} = \\frac{1}{9}log_2 (p(there|<s>) * p(was|there) * p(no|was) * p(<unk>|no) * p(behind|<unk>) * p(them|behind) * p(.|them) * p</s>|.))$<br>\n",
    "$= \\frac{1}{9}log_2 (\\frac{c(there|<s>)+1}{c(<s>)+|V|} * \\frac{c(was|there)+1}{c(there)+|V|} * \\frac{c(no|was)+1}{c(was)+|V|} * \\frac{c(<unk>|no)+1}{c(no)+|V|} * \\frac{c(behind|<unk>)+1}{c(<unk>)+|V|} * \\frac{c(them|behind)+1}{c(behind)+|V|} * \\frac{c(.|them)+1}{c(them)+|V|} * \\frac{c(</s>|.)+1}{c(.)+|V|})$<br><br>\n",
    "\n",
    "$l_{s3} = \\frac{1}{10}log_2 (p(i|<s>) * p(look|i) * p(forward|look) * p(to|forward) * p(hearing|to) * p(your|hearing) * p(reply|your) * p(.|reply) * p(</s>|.))$<br>\n",
    "$= \\frac{1}{10}log_2 (\\frac{c(i|<s>)+1}{c(<s>)+|V|} * \\frac{c(look|i)+1}{c(i)+|V|} * \\frac{c(forward|look)+1}{c(look)+|V|} * \\frac{c(to|forward)+1}{c(forward)+|V|} * \\frac{1}{c(to)+|V|} * \\frac{1}{c(hearing)+|V|} * \\frac{c(reply|your)+1}{c(your)+|V|} * \\frac{c(.|reply)+1}{c(reply)+|V|} * \\frac{c(</s>|.)+1}{c(.)+|V|})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The add-one log probability for ('<s>', 'he') is -4.265\n",
      "The add-one log probability for ('he', 'was') is -4.921\n",
      "The add-one log probability for ('was', 'laughed') is -14.301\n",
      "The add-one log probability for ('laughed', 'off') is -13.88\n",
      "The add-one log probability for ('off', 'the') is -7.666\n",
      "The add-one log probability for ('the', 'screen') is -13.276\n",
      "The add-one log probability for ('screen', '.') is -13.877\n",
      "The add-one log probability for ('.', '</s>') is -0.745\n",
      "The log probability of sentence 1 in the Bigram Add-One Model is: -8.103\n",
      "The perplexity of sentence 1 in the Bigram Add-One Model is: 275.0\n",
      "---------------------------------------------------------------------\n",
      "The add-one log probability for ('<s>', 'there') is -6.755\n",
      "The add-one log probability for ('there', 'was') is -5.413\n",
      "The add-one log probability for ('was', 'no') is -7.382\n",
      "The add-one log probability for ('no', '<unk>') is -9.161\n",
      "The add-one log probability for ('<unk>', 'behind') is -12.201\n",
      "The add-one log probability for ('behind', 'them') is -10.432\n",
      "The add-one log probability for ('them', '.') is -6.843\n",
      "The add-one log probability for ('.', '</s>') is -0.745\n",
      "The log probability of sentence 2 in the Bigram Add-One Model is: -6.548\n",
      "The perplexity of sentence 2 in the Bigram Add-One Model is: 93.6\n",
      "---------------------------------------------------------------------\n",
      "The add-one log probability for ('<s>', 'i') is -5.484\n",
      "The add-one log probability for ('i', 'look') is -13.157\n",
      "The add-one log probability for ('look', 'forward') is -11.576\n",
      "The add-one log probability for ('forward', 'to') is -10.073\n",
      "The add-one log probability for ('to', 'hearing') is -14.599\n",
      "The add-one log probability for ('hearing', 'your') is -13.879\n",
      "The add-one log probability for ('your', 'reply') is -12.91\n",
      "The add-one log probability for ('reply', '.') is -11.071\n",
      "The add-one log probability for ('.', '</s>') is -0.745\n",
      "The log probability of sentence 3 in the Bigram Add-One Model is: -9.349\n",
      "The perplexity of sentence 3 in the Bigram Add-One Model is: 652.3\n",
      "---------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "s1_aopred = testing.add_one_predict(s1_bigrams, add_one_probs, train_dict, len(s1_unk))\n",
    "print('The log probability of sentence 1 in the Bigram Add-One Model is: {:.3f}'.format(s1_aopred))\n",
    "print('The perplexity of sentence 1 in the Bigram Add-One Model is: {:.1f}'.format(perplexity(s1_aopred)))\n",
    "print('---------------------------------------------------------------------')\n",
    "s2_aopred = testing.add_one_predict(s2_bigrams, add_one_probs, train_dict, len(s2_unk))\n",
    "print('The log probability of sentence 2 in the Bigram Add-One Model is: {:.3f}'.format(s2_aopred))\n",
    "print('The perplexity of sentence 2 in the Bigram Add-One Model is: {:.1f}'.format(perplexity(s2_aopred)))\n",
    "print('---------------------------------------------------------------------')\n",
    "s3_aopred = testing.add_one_predict(s3_bigrams, add_one_probs, train_dict, len(s3_unk))\n",
    "print('The log probability of sentence 3 in the Bigram Add-One Model is: {:.3f}'.format(s3_aopred))\n",
    "print('The perplexity of sentence 3 in the Bigram Add-One Model is: {:.1f}'.format(perplexity(s3_aopred)))\n",
    "print('---------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Maroon\">Part D: Bigram Model with Discounting and Katz Backoff</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, I build set A of the Katz backoff\n",
    "# Please see training.py for the code\n",
    "katz_unigrams, bigrams_A = training.build_set_A(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I do not actually build set B, but instead make an assumption that\n",
    "# the probability mass of set B is the count of all tokens in set A that appear in the corpus\n",
    "# subtracted from all the tokens in the corpus minus the word itself\n",
    "# Please see training.py for the code\n",
    "katz_probs = training.katz_backoff(train_list, katz_unigrams, bigrams_A, len(train_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Parameters for Each Sentence would also be the same as the Bigram ML Model:\n",
    "The big difference lies in calculating the probabilities, which differ for observed bigrams (which gives back the discounted probabilities) vs. unseen bigrams (which gives back a proportion of the leftover probabilities $alpha$).<br>\n",
    "However, the parameters for calculating the probabilities of each parameter requires the unigram counts for the tokens.<br>\n",
    "\n",
    "$\\alpha = 1 - \\sum{firstwordbigrams.in.set.A} \\frac{disc. bigram.probs}{unigram.count(first.word)}$\n",
    "\n",
    "$l_{s1} = \\frac{1}{9}log_2 (p(he|<s>) * p(was|he) * p(laughed|was) * p(off|laughed) * p(the|off) * p(screen|the) + p(.|screen) * p(</s>|.))$<br>\n",
    "$ = \\frac{1}{9}log_2 (\\frac{c(he|<s>)-0.5}{c(<s>)} * \\frac{c(was|he)-0.5}{c(he)} * \\alpha(was)\\frac{c(laughed)}{c(set.B)} * \\alpha(laughed)\\frac{c(was)}{\\sum c(set.B)} * \\frac{c(the|off)+1}{c(off)+|V|} * \\frac{c(screen|the)}{c(the)} + \\alpha(screen)\\frac{c(.)}{c(set.B)} * \\frac{c(</s>|.)}{c(.)})$<br><br>\n",
    "\n",
    "$l_{s2} = \\frac{1}{9}log_2 (p(there|<s>) * p(was|there) * p(no|was) * p(<unk>|no) * p(behind|<unk>) * p(them|behind) * p(.|them) * p</s>|.))$<br>\n",
    "$= \\frac{1}{9}log_2 (\\frac{c(there|<s>)-0.5}{c(<s>)} * \\frac{c(was|there)-0.5}{c(there)} * \\frac{c(no|was)-0.5}{c(was)} * \\frac{c(<unk>|no)-0.5}{c(no)} * \\frac{c(behind|<unk>)-0.5}{c(<unk>)} * \\frac{c(them|behind)-0.5}{c(behind)} * \\frac{c(.|them)-0.5}{c(them)} * \\frac{c(</s>|.)-0.5}{c(.)})$<br><br>\n",
    "\n",
    "$l_{s3} = \\frac{1}{10}log_2 (p(i|<s>) * p(look|i) * p(forward|look) * p(to|forward) * p(hearing|to) * p(your|hearing) * p(reply|your) * p(.|reply) * p(</s>|.))$<br>\n",
    "$= \\frac{1}{10}log_2 (\\frac{c(i|<s>)-0.5}{c(<s>)} * \\frac{c(look|i)-0.5}{c(i)} * \\frac{c(forward|look)-0.5}{c(look)} * \\frac{c(to|forward)-0.5}{c(forward)} * \\alpha(to)\\frac{c(hearing)}{c(set.B)} * \\alpha(hearing)\\frac{c(your)}{c(set.B)} * \\frac{c(reply|your)-0.5}{c(your)} * \\frac{c(.|reply)-0.5}{c(reply)} * \\frac{c(</s>|.)-0.5}{c(.)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Katz log probability for ('<s>', 'he') in Set A is -3.608\n",
      "The Katz log probability for ('he', 'was') in Set A is -3.107\n",
      "The Katz log probability for ('was', 'laughed') in set B is -11.363\n",
      "The Katz log probability for ('laughed', 'off') in set B is -8.805\n",
      "The Katz log probability for ('off', 'the') in Set A is -2.432\n",
      "The Katz log probability for ('the', 'screen') in Set A is -13.268\n",
      "The Katz log probability for ('screen', '.') in set B is -4.113\n",
      "The Katz log probability for ('.', '</s>') in Set A is -0.0\n",
      "The log probability of sentence 1 in the Bigram Katz Backoff Model is: -5.188\n",
      "The perplexity of sentence 1 in the Bigram Katz Backoff Model is: 36.5\n",
      "---------------------------------------------------------------------\n",
      "The Katz log probability for ('<s>', 'there') in Set A is -6.102\n",
      "The Katz log probability for ('there', 'was') in Set A is -1.708\n",
      "The Katz log probability for ('was', 'no') in Set A is -5.429\n",
      "The Katz log probability for ('no', '<unk>') in Set A is -5.235\n",
      "The Katz log probability for ('<unk>', 'behind') in Set A is -11.52\n",
      "The Katz log probability for ('behind', 'them') in Set A is -4.127\n",
      "The Katz log probability for ('them', '.') in Set A is -2.573\n",
      "The Katz log probability for ('.', '</s>') in Set A is -0.0\n",
      "The log probability of sentence 2 in the Bigram Katz Backoff Model is: -4.077\n",
      "The perplexity of sentence 2 in the Bigram Katz Backoff Model is: 16.9\n",
      "---------------------------------------------------------------------\n",
      "The Katz log probability for ('<s>', 'i') in Set A is -4.828\n",
      "The Katz log probability for ('i', 'look') in Set A is -12.66\n",
      "The Katz log probability for ('look', 'forward') in Set A is -6.044\n",
      "The Katz log probability for ('forward', 'to') in Set A is -1.911\n",
      "The Katz log probability for ('to', 'hearing') in set B is -12.094\n",
      "The Katz log probability for ('hearing', 'your') in set B is -8.364\n",
      "The Katz log probability for ('your', 'reply') in Set A is -9.52\n",
      "The Katz log probability for ('reply', '.') in Set A is -2.399\n",
      "The Katz log probability for ('.', '</s>') in Set A is -0.0\n",
      "The log probability of sentence 3 in the Bigram Katz Backoff Model is: -6.424\n",
      "The perplexity of sentence 3 in the Bigram Katz Backoff Model is: 85.9\n",
      "---------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "s1_kpred = testing.katz_predict(s1_bigrams, katz_probs, katz_unigrams, bigrams_A, len(train_list), len(s1_unk))\n",
    "print('The log probability of sentence 1 in the Bigram Katz Backoff Model is: {:.3f}'.format(s1_kpred))\n",
    "print('The perplexity of sentence 1 in the Bigram Katz Backoff Model is: {:.1f}'.format(perplexity(s1_kpred)))\n",
    "print('---------------------------------------------------------------------')\n",
    "s2_kpred = testing.katz_predict(s2_bigrams, katz_probs, katz_unigrams, bigrams_A, len(train_list), len(s1_unk))\n",
    "print('The log probability of sentence 2 in the Bigram Katz Backoff Model is: {:.3f}'.format(s2_kpred))\n",
    "print('The perplexity of sentence 2 in the Bigram Katz Backoff Model is: {:.1f}'.format(perplexity(s2_kpred)))\n",
    "print('---------------------------------------------------------------------')\n",
    "s3_kpred = testing.katz_predict(s3_bigrams, katz_probs, katz_unigrams, bigrams_A, len(train_list), len(s1_unk))\n",
    "print('The log probability of sentence 3 in the Bigram Katz Backoff Model is: {:.3f}'.format(s3_kpred))\n",
    "print('The perplexity of sentence 3 in the Bigram Katz Backoff Model is: {:.1f}'.format(perplexity(s3_kpred)))\n",
    "print('---------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center;\">Summary</h4>\n",
    "\n",
    "<p style=\"text-align: center;\"><b>Log Probabilities and Perplexities by Model</b></p>\n",
    "<p style=\"text-align: center;\"><b>Sentence 1</b></p>\n",
    "\n",
    "| Model                                    | Log Probability | Perplexity       |\n",
    "|------------------------------------------|-----------------|------------------|\n",
    "| Unigram Maximum Likelihood               | -7.681          | 205.2            |\n",
    "| Bigram Maximum Likelihood                | -Infinite       | Infinite         |\n",
    "| Bigram with Add-One Smoothing            | -8.103          | 275.0            |\n",
    "| Bigram with Discounting and Katz Backoff | -5.188          | 36.5             |\n",
    "\n",
    "<p style=\"text-align: center;\"><b>Sentence 2</b></p>\n",
    "\n",
    "| Model                                    | Log Probability | Perplexity       |\n",
    "|------------------------------------------|-----------------|------------------|\n",
    "| Unigram Maximum Likelihood               | -7.030          | 130.7            |\n",
    "| Bigram Maximum Likelihood                | -4.047          | 16.5             |\n",
    "| Bigram with Add-One Smoothing            | -6.548          | 93.6             |\n",
    "| Bigram with Discounting and Katz Backoff | -4.077          | 16.9             |\n",
    "\n",
    "<p style=\"text-align: center;\"><b>Sentence 3</b></p>\n",
    "\n",
    "| Model                                    | Log Probability | Perplexity       |\n",
    "|------------------------------------------|-----------------|------------------|\n",
    "| Unigram Maximum Likelihood               | -8.889          | 474.1            |\n",
    "| Bigram Maximum Likelihood                | -Infinite       | Infinite         |\n",
    "| Bigram with Add-One Smoothing            | -9.349          | 652.3            |\n",
    "| Bigram with Discounting and Katz Backoff | -6.424          | 85.9             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:LightCoral\"> Question 7:</span>\n",
    "#### <em>Compute the perplexities of the entire test corpora, separately for the brown-test.txt and learner-test.txt under each of the models. Discuss the differences in the results you obtained.</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For easier viewing, I recreate the steps above to preprocess and train all the models again\n",
    "# 1. Training data preprocessing\n",
    "train_fp = 'data/brown-train.txt'\n",
    "train_list = preprocessing.preprocess_train(train_fp)\n",
    "train_dict = preprocessing.build_dict(train_list)\n",
    "\n",
    "# Bigrams list and dictionary with counts for training data\n",
    "train_bigrams = training.bigrams(train_list)\n",
    "train_bdict = training.bigram_dict(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Test data preprocessing\n",
    "test_fp = 'data/brown-test.txt'\n",
    "learner_fp = 'data/learner-test.txt'\n",
    "\n",
    "test_list = preprocessing.preprocess_test(test_fp, train_dict)\n",
    "learner_list = preprocessing.preprocess_test(learner_fp, train_dict)\n",
    "\n",
    "# Bigrams lists and dictionaries with counts\n",
    "test_bigrams = training.bigrams(test_list)\n",
    "learner_bigrams = training.bigrams(learner_list)\n",
    "\n",
    "test_bdict = training.bigram_dict(test_list)\n",
    "learner_bdict = training.bigram_dict(learner_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Unigram ML Model\n",
    "# to create dictionary of tokens and probabilities\n",
    "unigram_probs = training.unigram_train(train_list, train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Bigram ML Model\n",
    "# to create dictionary of bigrams and probabilities\n",
    "bigram_probs = training.bigram_train(train_bdict, train_list, train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Bigram with Add-One Smoothing\n",
    "add_one_probs = training.add_one_train(train_bdict, train_list, train_dict, len(train_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Bigram with Discounting and Katz Backoff\n",
    "katz_unigrams, bigrams_A = training.build_set_A(train_list)\n",
    "katz_probs = training.katz_backoff(train_list, katz_unigrams, bigrams_A, len(train_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Maroon\">Part A: brown-test.txt</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of the Unigram ML Model on brown-test.txt: 365.1\n"
     ]
    }
   ],
   "source": [
    "# Unigram ML Model on brown-test.txt\n",
    "test_unigram = testing.unigram_predict_no_print(test_list, unigram_probs)\n",
    "test_uni_perplexity = perplexity(test_unigram)\n",
    "print('The perplexity of the Unigram ML Model on brown-test.txt: {:.1f}'.format(test_uni_perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of the Bigram ML Model on brown-test.txt: Too Large or Infinite\n"
     ]
    }
   ],
   "source": [
    "# Bigram ML Model on brown-test.txt\n",
    "test_bigram = testing.bigram_predict_file(test_fp, bigram_probs, train_dict, len(test_list))\n",
    "test_bi_perplexity = perplexity(test_bigram)\n",
    "print('The perplexity of the Bigram ML Model on brown-test.txt:',test_bi_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of the Bigram Add-One Smoothing Model on brown-test.txt: 668.7\n"
     ]
    }
   ],
   "source": [
    "# Bigram Model with Add-One Smoothing on brown-test.txt\n",
    "test_add_one = testing.add_one_predict_no_print(test_bigrams, add_one_probs, train_dict, len(test_list))\n",
    "test_ao_perplexity = perplexity(test_add_one)\n",
    "print('The perplexity of the Bigram Add-One Smoothing Model on brown-test.txt: {:.1f}'.format(test_ao_perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of the Bigram Katz Backoff Model on brown-test.txt: 78.8\n"
     ]
    }
   ],
   "source": [
    "# Bigram Model with Discounting and Katz Backoff\n",
    "test_katz = testing.katz_predict_no_print(test_bigrams, katz_probs, katz_unigrams, bigrams_A, \n",
    "                                     len(train_list), len(test_list))\n",
    "test_katz_perplexity = perplexity(test_katz)\n",
    "print('The perplexity of the Bigram Katz Backoff Model on brown-test.txt: {:.1f}'.format(test_katz_perplexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:Maroon\">Part B: learner-test.txt</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of the Unigram ML Model on learner-test.txt: 409.6\n"
     ]
    }
   ],
   "source": [
    "# Unigram ML Model on learner-test.txt\n",
    "learner_unigram = testing.unigram_predict_no_print(learner_list, unigram_probs)\n",
    "learner_uni_perplexity = perplexity(learner_unigram)\n",
    "print('The perplexity of the Unigram ML Model on learner-test.txt: {:.1f}'.format(learner_uni_perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of the Bigram ML Model on learner-test.txt:  Too Large or Infinite\n"
     ]
    }
   ],
   "source": [
    "# Bigram ML Model on learner-test.txt\n",
    "learner_bigram = testing.bigram_predict_file(learner_fp, bigram_probs, train_dict, len(learner_list))\n",
    "learner_bi_perplexity = perplexity(learner_bigram)\n",
    "print('The perplexity of the Bigram ML Model on learner-test.txt: ', learner_bi_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of the Bigram Add-One Smoothing Model on learner-test.txt: 845.5\n"
     ]
    }
   ],
   "source": [
    "# Bigram Model with Add-One Smoothing on brown-test.txt\n",
    "learner_add_one = testing.add_one_predict_no_print(learner_bigrams, add_one_probs, train_dict, len(learner_list))\n",
    "learner_ao_perplexity = perplexity(learner_add_one)\n",
    "print('The perplexity of the Bigram Add-One Smoothing Model on learner-test.txt: {:.1f}'.format(learner_ao_perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity of the Bigram Katz Backoff Model on learner-test.txt: 88.8\n"
     ]
    }
   ],
   "source": [
    "# Bigram Model with Discounting and Katz Backoff\n",
    "learner_katz = testing.katz_predict_no_print(learner_bigrams, katz_probs, katz_unigrams, bigrams_A, \n",
    "                                        len(train_list), len(learner_list))\n",
    "learner_katz_perplexity = perplexity(learner_katz)\n",
    "print('The perplexity of the Bigram Katz Backoff Model on learner-test.txt: {:.1f}'.format(learner_katz_perplexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:center;\">Summary</h4>\n",
    "\n",
    "<p style=\"text-align: center;\"><b>Perplexities by Model for Two Test Data Sets</b></p>\n",
    "\n",
    "| Model                                    | brown-test.txt | learner-test.txt |\n",
    "|------------------------------------------|----------------|------------------|\n",
    "| Unigram Maximum Likelihood               | 365.1          | 409.6            |\n",
    "| Bigram Maximum Likelihood                | Infinite       | Infinite         |\n",
    "| Bigram with Add-One Smoothing            | 668.7          | 845.5            |\n",
    "| Bigram with Discounting and Katz Backoff | 78.8           | 88.8             |\n",
    "\n",
    "In general, the brown-test had lower perplexities (i.e. was better modeled) than the learner-test.\n",
    "I believe three factors may help explain this:\n",
    "1. The brown-test comes from the same source, and barring any strange data design choices, natural data from the same source should have more similar patterns and structure than if it were otherwise.\n",
    "2. The learner-test contains many letters, as opposed to the brown-test, which has many spoken quotations. The differences between spoken language and written language can also account for the difference in perplexities. As the brown-train has more spoken language, the brown-test would have lower perplexity and vice versa for learner-test. This is observed in Questions 5 and 6, where the training set predicted sentence 3 poorly compared to the other two sentences.\n",
    "3. Finally, because the learner-test is written by non-native speakers of English, you can find differences not normally found in native speech patterns: (e.g. \"it should started\"). This would raise the perplexity of the learner-test as there would be, on average, more unseen tokens and bigrams."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
